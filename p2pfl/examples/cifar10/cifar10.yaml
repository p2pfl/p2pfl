description: "Example of using P2PFL with the CIFAR-10 dataset, a collection of 32x32 color images in 10 classes."

# Remote loggers configuration (optional)
remote_loggers:
  # wandb_api_key: "your-api-key"
  # wandb_project: "p2pfl-cifar10"
  # p2pfl_web_url: "https://your-server.com"
  # p2pfl_web_key: "your-api-key"

# Defines network setup
network:
  package: "p2pfl.communication.protocols.protobuff.memory"
  protocol: "MemoryCommunicationProtocol"
  nodes: 8
  topology: "star"   # Network topology: "line", "ring", "star", "full", null for custom
  additional_connections:
  #  - [0, 2]              # Connect node 0 to Node 2 (indices)
  #  - [1, 3]              # Connect node 1 to Node 3 (indices)

# Defines experiment settings
experiment:
  name: "cifar10_experiment"
  rounds: 130
  epochs: 1
  trainset_size: 4
  seed: 666
  wait_timeout: 300  # Timeout in minutes for wait_to_finish

  # Dataset settings
  dataset:
    source: "huggingface" # Dataset source: "huggingface", "csv", etc.
    name: "p2pfl/CIFAR10"
    batch_size: 256
    # Use our custom CIFAR10 transform function (normalization only, no data augmentation)
    transforms:
      package: "p2pfl.examples.cifar10.transforms"
      function: "get_cifar10_transforms"
      params: {}
    partitioning:
      package: "p2pfl.learning.dataset.partition_strategies"
      strategy: "RandomIIDPartitionStrategy" # DirichletPartitionStrategy
      reduced_dataset: false  # For testing purposes
      reduction_factor: 10

      params: # Partitioning parameters (strategy-specific)
        # Dirichlet example
        alpha: 0.3
        min_partition_size: 5
        self_balancing: false

  # Learning algorithm configuration
  model:
    package: "p2pfl.examples.cifar10.model.resnet_pytorch" 
    model_build_fn: "model_build_fn"
    params: {}  
    # Learning algorithm parameters (algorithm-specific)
    #compression:
    #ptq:
    #  dtype: "int8"
    #topk:
    #  k: 0.1
    #zlib:
    #  level: 6

  # Aggregation algorithm configuration
  aggregator:
    package: "p2pfl.learning.aggregators.fedavg"
    aggregator: "FedAvg"
    params: {}  # Aggregator parameters (algorithm-specific)
    # Optional transforms for preprocessing
    transforms:
      package: "p2pfl.examples.cifar10.transforms"
      function: "cifar10_transforms"
      #params:
        # Optional parameters for the transforms
        
# General experiment settings
settings:
  general:
    log_level: "INFO" # Logging verbosity: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
    grpc_timeout: 2
    disable_ray: False
    resource_monitor_period: 1

  SSL:
    use_ssl: false
    CA_CRT: None
    SERVER_CRT: None
    CLIENT_CRT: None
    SERVER_KEY: None
    CLIENT_KEY: None
  heartbeat:
    period: 10
    timeout: 60
    wait_convergence: 4
    exclude_beat_logs: true
  gossip:
    period: 1
    ttl: 10
    messages_per_period: 200
    amount_last_messages_saved: 1000
    models_period: 1
    models_per_round: 4
    exit_on_x_equal_rounds: 150
  training:
    vote_timeout: 60
    aggregation_timeout: 300
    ray_actor_pool_size: 1

# Profiling configuration
profiling:
  enabled: false
  measure_time: true
  output_dir: "profile/cifar10"
