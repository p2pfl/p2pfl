{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Experiment Results Analysis\n",
    "\n",
    "Analyze MNIST federated learning experiment results with paper-ready visualizations.\n",
    "\n",
    "**Usage**: Set the `experiment_path` variable to point to your experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set plotting parameters for paper-ready figures\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 13\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Experiment Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET YOUR EXPERIMENT PATH HERE\n",
    "experiment_path = '../../../results/mnist-experiment-1'\n",
    "\n",
    "# Convert to Path object\n",
    "exp_path = Path(experiment_path)\n",
    "\n",
    "# Check if path exists\n",
    "if exp_path.exists():\n",
    "    print(f\"✓ Experiment path found: {exp_path.absolute()}\")\n",
    "    \n",
    "    # List available files\n",
    "    files = list(exp_path.glob('*'))\n",
    "    print(f\"\\nFiles in experiment directory:\")\n",
    "    for f in files[:10]:  # Show first 10 files\n",
    "        print(f\"  - {f.name}\")\n",
    "    if len(files) > 10:\n",
    "        print(f\"  ... and {len(files)-10} more files\")\n",
    "else:\n",
    "    print(f\"✗ Experiment path not found: {exp_path}\")\n",
    "    print(\"Please update the 'experiment_path' variable to point to your experiment directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load metrics from experiment path\n",
    "def load_experiment_metrics(exp_path):\n",
    "    \"\"\"Load metrics from experiment directory.\"\"\"\n",
    "    metrics_file = exp_path / 'global_metrics.csv'\n",
    "    \n",
    "    if not metrics_file.exists():\n",
    "        print(f\"Warning: global_metrics.csv not found in {exp_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(metrics_file)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metrics: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load the metrics\n",
    "df_metrics = load_experiment_metrics(exp_path)\n",
    "\n",
    "if not df_metrics.empty:\n",
    "    print(f\"✓ Loaded {len(df_metrics)} records\")\n",
    "    print(f\"\\nMetrics available: {df_metrics['metric'].unique().tolist()}\")\n",
    "    print(f\"Rounds: {df_metrics['round'].min()} - {df_metrics['round'].max()}\")\n",
    "    print(f\"Number of nodes: {df_metrics['node'].nunique()}\")\n",
    "    print(f\"Nodes: {sorted(df_metrics['node'].unique())}\")\n",
    "else:\n",
    "    print(\"No metrics data loaded. Please check your experiment path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Metrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the metrics data\n",
    "if not df_metrics.empty:\n",
    "    # Identify the correct metric names for loss and accuracy\n",
    "    # Common metric names in P2PFL:\n",
    "    loss_metrics = ['loss', 'train_loss', 'test_loss']\n",
    "    acc_metrics = ['accuracy', 'acc', 'test_accuracy', 'test_acc', 'test_metric', 'compile_metrics']\n",
    "    \n",
    "    # Find which metrics are available\n",
    "    available_metrics = df_metrics['metric'].unique()\n",
    "    loss_metric = None\n",
    "    acc_metric = None\n",
    "    \n",
    "    for metric in loss_metrics:\n",
    "        if metric in available_metrics:\n",
    "            loss_metric = metric\n",
    "            break\n",
    "    \n",
    "    for metric in acc_metrics:\n",
    "        if metric in available_metrics:\n",
    "            acc_metric = metric\n",
    "            break\n",
    "    \n",
    "    print(f\"Using loss metric: {loss_metric}\")\n",
    "    print(f\"Using accuracy metric: {acc_metric}\")\n",
    "    \n",
    "    # Calculate average metrics per round\n",
    "    avg_loss = pd.DataFrame()\n",
    "    avg_accuracy = pd.DataFrame()\n",
    "    \n",
    "    if loss_metric:\n",
    "        loss_df = df_metrics[df_metrics['metric'] == loss_metric]\n",
    "        avg_loss = loss_df.groupby('round')['value'].agg(['mean', 'std', 'count']).reset_index()\n",
    "        avg_loss.columns = ['round', 'mean', 'std', 'count']\n",
    "        avg_loss = avg_loss.sort_values('round')\n",
    "        print(f\"\\nLoss data: {len(avg_loss)} rounds\")\n",
    "    \n",
    "    if acc_metric:\n",
    "        acc_df = df_metrics[df_metrics['metric'] == acc_metric]\n",
    "        avg_accuracy = acc_df.groupby('round')['value'].agg(['mean', 'std', 'count']).reset_index()\n",
    "        avg_accuracy.columns = ['round', 'mean', 'std', 'count']\n",
    "        avg_accuracy = avg_accuracy.sort_values('round')\n",
    "        print(f\"Accuracy data: {len(avg_accuracy)} rounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Visualization: Loss and Accuracy Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined plot with MNIST grid styling\n",
    "if not df_metrics.empty and not avg_loss.empty and not avg_accuracy.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Define colors from colormap\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(avg_loss['round'], avg_loss['mean'], \n",
    "             color=colors[0], linewidth=2.5, marker='o', markersize=6, \n",
    "             markevery=max(1, len(avg_loss)//10), label='Mean Loss', alpha=0.9)\n",
    "    \n",
    "    # Add shaded area for std deviation\n",
    "    ax1.fill_between(avg_loss['round'], \n",
    "                     avg_loss['mean'] - avg_loss['std'],\n",
    "                     avg_loss['mean'] + avg_loss['std'],\n",
    "                     alpha=0.3, color=colors[0], edgecolor='none')\n",
    "    \n",
    "    ax1.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Average Loss', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Loss Evolution', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax1.legend(loc='upper right', framealpha=0.9, edgecolor='black', fancybox=True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(avg_accuracy['round'], avg_accuracy['mean'],\n",
    "             color=colors[2], linewidth=2.5, marker='s', markersize=6,\n",
    "             markevery=max(1, len(avg_accuracy)//10), label='Mean Accuracy', alpha=0.9)\n",
    "    \n",
    "    # Add shaded area for std deviation\n",
    "    ax2.fill_between(avg_accuracy['round'], \n",
    "                     avg_accuracy['mean'] - avg_accuracy['std'],\n",
    "                     avg_accuracy['mean'] + avg_accuracy['std'],\n",
    "                     alpha=0.3, color=colors[2], edgecolor='none')\n",
    "    \n",
    "    ax2.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Average Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Accuracy Evolution', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax2.legend(loc='lower right', framealpha=0.9, edgecolor='black', fancybox=True)\n",
    "    \n",
    "    # Add experiment name to title\n",
    "    exp_name = exp_path.name if exp_path.name else 'MNIST'\n",
    "    plt.suptitle(f'{exp_name} Federated Learning Performance', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif not df_metrics.empty:\n",
    "    # If we only have one metric, plot it\n",
    "    if not avg_loss.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, 10))\n",
    "        \n",
    "        ax.plot(avg_loss['round'], avg_loss['mean'], \n",
    "                color=colors[0], linewidth=2.5, marker='o', markersize=6, \n",
    "                markevery=max(1, len(avg_loss)//10), label='Mean Loss', alpha=0.9)\n",
    "        \n",
    "        ax.fill_between(avg_loss['round'], \n",
    "                        avg_loss['mean'] - avg_loss['std'],\n",
    "                        avg_loss['mean'] + avg_loss['std'],\n",
    "                        alpha=0.3, color=colors[0], edgecolor='none')\n",
    "        \n",
    "        ax.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Average Loss', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Loss Evolution', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "        ax.legend(loc='upper right', framealpha=0.9, edgecolor='black', fancybox=True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    elif not avg_accuracy.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, 10))\n",
    "        \n",
    "        ax.plot(avg_accuracy['round'], avg_accuracy['mean'],\n",
    "                color=colors[2], linewidth=2.5, marker='s', markersize=6,\n",
    "                markevery=max(1, len(avg_accuracy)//10), label='Mean Accuracy', alpha=0.9)\n",
    "        \n",
    "        ax.fill_between(avg_accuracy['round'], \n",
    "                        avg_accuracy['mean'] - avg_accuracy['std'],\n",
    "                        avg_accuracy['mean'] + avg_accuracy['std'],\n",
    "                        alpha=0.3, color=colors[2], edgecolor='none')\n",
    "        \n",
    "        ax.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Average Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Accuracy Evolution', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "        ax.legend(loc='lower right', framealpha=0.9, edgecolor='black', fancybox=True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting. Please check your experiment path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Node Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual node performance\n",
    "if not df_metrics.empty:\n",
    "    # Determine number of subplots needed\n",
    "    n_plots = 0\n",
    "    if loss_metric and not df_metrics[df_metrics['metric'] == loss_metric].empty:\n",
    "        n_plots += 1\n",
    "    if acc_metric and not df_metrics[df_metrics['metric'] == acc_metric].empty:\n",
    "        n_plots += 1\n",
    "    \n",
    "    if n_plots > 0:\n",
    "        fig, axes = plt.subplots(1, n_plots, figsize=(7*n_plots, 6))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        plot_idx = 0\n",
    "        \n",
    "        # Loss per node\n",
    "        if loss_metric and not df_metrics[df_metrics['metric'] == loss_metric].empty:\n",
    "            ax = axes[plot_idx]\n",
    "            loss_df = df_metrics[df_metrics['metric'] == loss_metric]\n",
    "            nodes = sorted(loss_df['node'].unique(), \n",
    "                          key=lambda x: int(x.split('-')[-1]) if '-' in x and x.split('-')[-1].isdigit() else x)\n",
    "            \n",
    "            colors = plt.cm.tab10(np.linspace(0, 1, len(nodes)))\n",
    "            \n",
    "            for idx, node in enumerate(nodes):\n",
    "                node_data = loss_df[loss_df['node'] == node].sort_values('round')\n",
    "                ax.plot(node_data['round'], node_data['value'], \n",
    "                        marker='o', markersize=3, label=node, alpha=0.8, \n",
    "                        linewidth=2, color=colors[idx])\n",
    "            \n",
    "            ax.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Loss Evolution per Node', fontsize=14, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "            ax.legend(loc='best', ncol=min(2, (len(nodes)+3)//4), framealpha=0.95, \n",
    "                     edgecolor='black', fancybox=True, fontsize=9)\n",
    "            plot_idx += 1\n",
    "        \n",
    "        # Accuracy per node\n",
    "        if acc_metric and not df_metrics[df_metrics['metric'] == acc_metric].empty:\n",
    "            ax = axes[plot_idx]\n",
    "            acc_df = df_metrics[df_metrics['metric'] == acc_metric]\n",
    "            nodes = sorted(acc_df['node'].unique(), \n",
    "                          key=lambda x: int(x.split('-')[-1]) if '-' in x and x.split('-')[-1].isdigit() else x)\n",
    "            \n",
    "            colors = plt.cm.tab10(np.linspace(0, 1, len(nodes)))\n",
    "            \n",
    "            for idx, node in enumerate(nodes):\n",
    "                node_data = acc_df[acc_df['node'] == node].sort_values('round')\n",
    "                ax.plot(node_data['round'], node_data['value'], \n",
    "                        marker='s', markersize=3, label=node, alpha=0.8, \n",
    "                        linewidth=2, color=colors[idx])\n",
    "            \n",
    "            ax.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Accuracy Evolution per Node', fontsize=14, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "            ax.legend(loc='best', ncol=min(2, (len(nodes)+3)//4), framealpha=0.95,\n",
    "                     edgecolor='black', fancybox=True, fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics and final performance\n",
    "if not df_metrics.empty:\n",
    "    print(\"=\"*60)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Experiment Path: {exp_path.absolute()}\")\n",
    "    print(f\"Total Rounds: {df_metrics['round'].max()}\")\n",
    "    print(f\"Number of Nodes: {df_metrics['node'].nunique()}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL PERFORMANCE (Last Round)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    last_round = df_metrics['round'].max()\n",
    "    \n",
    "    # Loss statistics\n",
    "    if loss_metric:\n",
    "        final_loss = df_metrics[(df_metrics['metric'] == loss_metric) & \n",
    "                                (df_metrics['round'] == last_round)]['value']\n",
    "        if not final_loss.empty:\n",
    "            print(f\"\\nLoss Statistics:\")\n",
    "            print(f\"  Mean: {final_loss.mean():.6f}\")\n",
    "            print(f\"  Std:  {final_loss.std():.6f}\")\n",
    "            print(f\"  Min:  {final_loss.min():.6f}\")\n",
    "            print(f\"  Max:  {final_loss.max():.6f}\")\n",
    "    \n",
    "    # Accuracy statistics\n",
    "    if acc_metric:\n",
    "        final_acc = df_metrics[(df_metrics['metric'] == acc_metric) & \n",
    "                               (df_metrics['round'] == last_round)]['value']\n",
    "        if not final_acc.empty:\n",
    "            print(f\"\\nAccuracy Statistics:\")\n",
    "            print(f\"  Mean: {final_acc.mean():.4f}\")\n",
    "            print(f\"  Std:  {final_acc.std():.4f}\")\n",
    "            print(f\"  Min:  {final_acc.min():.4f}\")\n",
    "            print(f\"  Max:  {final_acc.max():.4f}\")\n",
    "            \n",
    "            # Best performing node\n",
    "            best_node_idx = final_acc.idxmax()\n",
    "            best_node = df_metrics.loc[best_node_idx, 'node']\n",
    "            print(f\"\\n  Best Node: {best_node} ({final_acc.max():.4f})\")\n",
    "            \n",
    "            # Worst performing node\n",
    "            worst_node_idx = final_acc.idxmin()\n",
    "            worst_node = df_metrics.loc[worst_node_idx, 'node']\n",
    "            print(f\"  Worst Node: {worst_node} ({final_acc.min():.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Over Time (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a performance heatmap if there are multiple nodes\n",
    "if not df_metrics.empty and df_metrics['node'].nunique() > 1:\n",
    "    if acc_metric and not df_metrics[df_metrics['metric'] == acc_metric].empty:\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Prepare data for heatmap\n",
    "        acc_df = df_metrics[df_metrics['metric'] == acc_metric]\n",
    "        nodes = sorted(acc_df['node'].unique(), \n",
    "                      key=lambda x: int(x.split('-')[-1]) if '-' in x and x.split('-')[-1].isdigit() else x)\n",
    "        rounds = sorted(acc_df['round'].unique())\n",
    "        \n",
    "        # Create matrix for heatmap\n",
    "        acc_matrix = np.zeros((len(nodes), len(rounds)))\n",
    "        for i, node in enumerate(nodes):\n",
    "            for j, round_num in enumerate(rounds):\n",
    "                val = acc_df[(acc_df['node'] == node) & (acc_df['round'] == round_num)]['value'].values\n",
    "                if len(val) > 0:\n",
    "                    acc_matrix[i, j] = val[0]\n",
    "        \n",
    "        # Create heatmap\n",
    "        im = ax.imshow(acc_matrix, aspect='auto', cmap='YlGn', interpolation='nearest')\n",
    "        \n",
    "        # Set ticks\n",
    "        ax.set_xticks(range(0, len(rounds), max(1, len(rounds)//20)))\n",
    "        ax.set_xticklabels([rounds[i] for i in range(0, len(rounds), max(1, len(rounds)//20))])\n",
    "        ax.set_yticks(range(len(nodes)))\n",
    "        ax.set_yticklabels(nodes)\n",
    "        \n",
    "        ax.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Node', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Accuracy Heatmap - Performance Over Time', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, label='Accuracy')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export summary statistics to CSV\n",
    "if not df_metrics.empty:\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    \n",
    "    for round_num in sorted(df_metrics['round'].unique()):\n",
    "        round_data = {'round': round_num}\n",
    "        \n",
    "        if loss_metric:\n",
    "            loss_values = df_metrics[(df_metrics['metric'] == loss_metric) & \n",
    "                                     (df_metrics['round'] == round_num)]['value']\n",
    "            if not loss_values.empty:\n",
    "                round_data['loss_mean'] = loss_values.mean()\n",
    "                round_data['loss_std'] = loss_values.std()\n",
    "        \n",
    "        if acc_metric:\n",
    "            acc_values = df_metrics[(df_metrics['metric'] == acc_metric) & \n",
    "                                    (df_metrics['round'] == round_num)]['value']\n",
    "            if not acc_values.empty:\n",
    "                round_data['acc_mean'] = acc_values.mean()\n",
    "                round_data['acc_std'] = acc_values.std()\n",
    "        \n",
    "        summary_data.append(round_data)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = exp_path / 'summary_statistics.csv'\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    print(f\"Summary statistics saved to: {output_file}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nSummary Statistics (first 5 rounds):\")\n",
    "    print(summary_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2pfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
