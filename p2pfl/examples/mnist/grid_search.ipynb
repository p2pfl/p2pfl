{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Configuration Rankings\n",
    "\n",
    "This notebook analyzes grid search experiments and ranks the best configurations for each federated learning aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import yaml\n",
    "from typing import Dict, List\n",
    "\n",
    "# Set plotting parameters for paper-ready figures\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n",
    "plt.rcParams[\"axes.titlesize\"] = 13\n",
    "plt.rcParams[\"xtick.labelsize\"] = 10\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10\n",
    "plt.rcParams[\"legend.fontsize\"] = 10\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Maximum Round for Analysis\n",
    "\n",
    "Set the maximum round to analyze. The analysis will stop at this round (between 1 and 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION: Set the maximum round for analysis\n",
    "MAX_ROUND = 30  # Change this value to stop analysis at a specific round (1-30)\n",
    "\n",
    "# Validate the max round\n",
    "if MAX_ROUND < 1:\n",
    "    MAX_ROUND = 1\n",
    "    print(f\"Warning: MAX_ROUND set to minimum value of 1\")\n",
    "elif MAX_ROUND > 30:\n",
    "    MAX_ROUND = 30\n",
    "    print(f\"Warning: MAX_ROUND capped at maximum value of 30\")\n",
    "\n",
    "print(f\"Analysis will be performed up to round {MAX_ROUND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parse Experiment Results with Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_experiment_dir(exp_dir: Path, max_round: int = None) -> Dict:\n",
    "    \"\"\"Parse a single experiment directory and extract metrics and parameters from config.yaml.\n",
    "\n",
    "    Args:\n",
    "        exp_dir: Path to experiment directory\n",
    "        max_round: Maximum round to consider for analysis (if None, use all rounds)\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # Parse directory name for basic info\n",
    "    dir_name = exp_dir.name\n",
    "\n",
    "    # Extract aggregator from directory name\n",
    "    agg_match = re.search(r\"aggregator_aggregator_(\\w+)\", dir_name)\n",
    "    if agg_match:\n",
    "        result[\"aggregator\"] = agg_match.group(1).split(\"_\")[0]\n",
    "\n",
    "    # Load parameters from config.yaml\n",
    "    config_file = exp_dir / \"config.yaml\"\n",
    "    if config_file.exists():\n",
    "        try:\n",
    "            with open(config_file, \"r\") as f:\n",
    "                config = yaml.safe_load(f)\n",
    "\n",
    "            # Extract aggregator parameters\n",
    "            if \"experiment\" in config and \"aggregator\" in config[\"experiment\"]:\n",
    "                agg_config = config[\"experiment\"][\"aggregator\"]\n",
    "\n",
    "                # Get aggregator name if not already found\n",
    "                if \"aggregator\" not in result and \"aggregator\" in agg_config:\n",
    "                    result[\"aggregator\"] = agg_config[\"aggregator\"]\n",
    "\n",
    "                # Extract parameters\n",
    "                if \"params\" in agg_config and agg_config[\"params\"]:\n",
    "                    for param, value in agg_config[\"params\"].items():\n",
    "                        result[param] = value\n",
    "\n",
    "            # Extract dataset partitioning parameters (alpha)\n",
    "            if \"experiment\" in config and \"dataset\" in config[\"experiment\"]:\n",
    "                if \"partitioning\" in config[\"experiment\"][\"dataset\"]:\n",
    "                    part_config = config[\"experiment\"][\"dataset\"][\"partitioning\"]\n",
    "                    if \"params\" in part_config and part_config[\"params\"]:\n",
    "                        if \"alpha\" in part_config[\"params\"]:\n",
    "                            result[\"alpha\"] = part_config[\"params\"][\"alpha\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading config from {exp_dir}: {e}\")\n",
    "\n",
    "    # Load metrics\n",
    "    metrics_file = exp_dir / \"global_metrics.csv\"\n",
    "    if metrics_file.exists():\n",
    "        try:\n",
    "            metrics_df = pd.read_csv(metrics_file)\n",
    "\n",
    "            # Filter metrics up to max_round if specified\n",
    "            if max_round is not None:\n",
    "                metrics_df = metrics_df[metrics_df[\"round\"] <= max_round]\n",
    "                result[\"max_round_used\"] = max_round\n",
    "\n",
    "            result[\"metrics\"] = metrics_df\n",
    "\n",
    "            # Calculate best accuracy achieved within the specified rounds\n",
    "            test_metrics = metrics_df[metrics_df[\"metric\"] == \"test_metric\"]\n",
    "            if not test_metrics.empty:\n",
    "                # Find the best accuracy achieved up to max_round\n",
    "                round_means = test_metrics.groupby(\"round\")[\"value\"].mean()\n",
    "                best_accuracy = round_means.max()\n",
    "                best_round = round_means.idxmax()\n",
    "\n",
    "                # Get metrics from the best round\n",
    "                best_round_metrics = test_metrics[test_metrics[\"round\"] == best_round]\n",
    "\n",
    "                result[\"final_accuracy\"] = best_accuracy  # Best accuracy achieved\n",
    "                result[\"final_accuracy_std\"] = best_round_metrics[\"value\"].std()\n",
    "                result[\"best_accuracy\"] = best_accuracy\n",
    "                result[\"best_round\"] = best_round  # Round where best accuracy was achieved\n",
    "                result[\"final_round\"] = test_metrics[\"round\"].max()  # Last available round\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading metrics from {exp_dir}: {e}\")\n",
    "\n",
    "    result[\"exp_dir\"] = exp_dir\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_all_experiments(base_dirs: List[str], max_round: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Load all experiments from multiple result directories.\n",
    "\n",
    "    Args:\n",
    "        base_dirs: List of directories containing experiment results\n",
    "        max_round: Maximum round to consider for analysis\n",
    "    \"\"\"\n",
    "    all_experiments = []\n",
    "\n",
    "    for base_dir in base_dirs:\n",
    "        base_path = Path(base_dir)\n",
    "        if not base_path.exists():\n",
    "            print(f\"Warning: {base_path} does not exist\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nLoading experiments from {base_path}...\")\n",
    "        exp_count = 0\n",
    "\n",
    "        # Find all experiment directories\n",
    "        for exp_dir in base_path.iterdir():\n",
    "            if exp_dir.is_dir() and not exp_dir.name.startswith(\".\"):\n",
    "                exp_data = parse_experiment_dir(exp_dir, max_round=max_round)\n",
    "                if \"final_accuracy\" in exp_data and \"aggregator\" in exp_data:\n",
    "                    all_experiments.append(exp_data)\n",
    "                    exp_count += 1\n",
    "\n",
    "        print(f\"  Found {exp_count} valid experiments\")\n",
    "\n",
    "    if not all_experiments:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_experiments)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define result directories\n",
    "result_dirs = [\n",
    "    \"../../../results_fedadagrad_grid\",\n",
    "    \"../../../results_fedadam_grid\",\n",
    "    \"../../../results_fedyogi_grid\",\n",
    "    #'../../../results_scaffold_grid',\n",
    "    \"../../../results_fedprox_grid\",\n",
    "    \"../../../results_all\",\n",
    "]\n",
    "\n",
    "# Load all experiments with the specified max round\n",
    "print(\"=\" * 80)\n",
    "print(f\"LOADING EXPERIMENTS (MAX ROUND: {MAX_ROUND})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "experiments_df = load_all_experiments(result_dirs, max_round=MAX_ROUND)\n",
    "\n",
    "if not experiments_df.empty:\n",
    "    print(f\"\\nTotal experiments loaded: {len(experiments_df)}\")\n",
    "    print(f\"Aggregators found: {sorted(experiments_df['aggregator'].unique())}\")\n",
    "    print(f\"\\nAnalysis limited to round {MAX_ROUND}\")\n",
    "    if \"final_round\" in experiments_df.columns:\n",
    "        actual_rounds = experiments_df[\"final_round\"].unique()\n",
    "        print(f\"Actual final rounds used: {sorted(actual_rounds)}\")\n",
    "else:\n",
    "    print(\"\\nNo experiments found. Please run the grid search commands first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rank Configurations Per Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_param_value(value):\n",
    "    \"\"\"Format parameter values for display.\"\"\"\n",
    "    if isinstance(value, float):\n",
    "        if value < 0.001:\n",
    "            return f\"{value:.2e}\"\n",
    "        else:\n",
    "            return f\"{value:.4f}\" if value < 1 else f\"{value:.2f}\"\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def print_ranked_configs(df: pd.DataFrame, aggregator: str, top_n: int = 5):\n",
    "    \"\"\"Print ranked configurations for a specific aggregator.\"\"\"\n",
    "    agg_df = df[df[\"aggregator\"] == aggregator].copy()\n",
    "\n",
    "    if agg_df.empty:\n",
    "        print(f\"No experiments found for {aggregator}\")\n",
    "        return\n",
    "\n",
    "    # Sort by final accuracy (descending)\n",
    "    agg_df = agg_df.sort_values(\"final_accuracy\", ascending=False)\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"RANKINGS FOR {aggregator.upper()} (UP TO ROUND {MAX_ROUND})\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total configurations tested: {len(agg_df)}\")\n",
    "    print(f\"Top {min(top_n, len(agg_df))} configurations:\\n\")\n",
    "\n",
    "    # Get parameter columns (exclude non-parameter columns)\n",
    "    exclude_cols = [\n",
    "        \"aggregator\",\n",
    "        \"final_accuracy\",\n",
    "        \"final_accuracy_std\",\n",
    "        \"best_accuracy\",\n",
    "        \"metrics\",\n",
    "        \"exp_dir\",\n",
    "        \"max_round_used\",\n",
    "        \"final_round\",\n",
    "    ]\n",
    "    param_cols = [col for col in agg_df.columns if col not in exclude_cols]\n",
    "\n",
    "    for rank, (idx, row) in enumerate(agg_df.head(top_n).iterrows(), 1):\n",
    "        print(f\"Rank #{rank}:\")\n",
    "        best_round = int(row.get(\"best_round\", row.get(\"final_round\", MAX_ROUND)))\n",
    "        print(f\"  Best accuracy: {row['final_accuracy']:.4f} (achieved at round {best_round})\", end=\"\")\n",
    "        if \"final_accuracy_std\" in row and not pd.isna(row[\"final_accuracy_std\"]) and row[\"final_accuracy_std\"] > 0:\n",
    "            print(f\" ± {row['final_accuracy_std']:.4f}\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "        print(\"  Parameters:\")\n",
    "        for param in sorted(param_cols):\n",
    "            if param in row and not pd.isna(row[param]):\n",
    "                print(f\"    {param}: {format_param_value(row[param])}\")\n",
    "        print()\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "# Print rankings for each aggregator\n",
    "if not experiments_df.empty:\n",
    "    ranked_dfs = {}\n",
    "\n",
    "    for aggregator in sorted(experiments_df[\"aggregator\"].unique()):\n",
    "        ranked_df = print_ranked_configs(experiments_df, aggregator, top_n=5)\n",
    "        if ranked_df is not None:\n",
    "            ranked_dfs[aggregator] = ranked_df\n",
    "else:\n",
    "    print(\"No experiments to rank.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Accuracy Per Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not experiments_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    best_accs = experiments_df.groupby(\"aggregator\")[\"final_accuracy\"].max().sort_values(ascending=False)\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(best_accs)))\n",
    "    bars = ax.bar(range(len(best_accs)), best_accs.values, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=1.5)\n",
    "\n",
    "    ax.set_xticks(range(len(best_accs)))\n",
    "    ax.set_xticklabels(best_accs.index, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "    ax.set_title(f\"Best Configuration Performance by Aggregator (Round {MAX_ROUND})\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, best_accs.values):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.001,\n",
    "            f\"{value:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Accuracy Distribution Across Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not experiments_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    agg_names = sorted(experiments_df[\"aggregator\"].unique())\n",
    "    acc_data = [experiments_df[experiments_df[\"aggregator\"] == agg][\"final_accuracy\"].values for agg in agg_names]\n",
    "\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(agg_names)))\n",
    "    bp = ax.boxplot(acc_data, labels=agg_names, patch_artist=True)\n",
    "\n",
    "    # Color the box plots\n",
    "    for patch, color in zip(bp[\"boxes\"], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    ax.set_xticklabels(agg_names, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "    ax.set_title(f\"Accuracy Distribution Across All Configurations (Round {MAX_ROUND})\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for box plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Curves for Best Configurations (Limited to Selected Round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not experiments_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(experiments_df[\"aggregator\"].unique())))\n",
    "\n",
    "    for idx, agg in enumerate(sorted(experiments_df[\"aggregator\"].unique())):\n",
    "        agg_df = experiments_df[experiments_df[\"aggregator\"] == agg]\n",
    "        if not agg_df.empty:\n",
    "            best_exp = agg_df.loc[agg_df[\"final_accuracy\"].idxmax()]\n",
    "\n",
    "            if \"metrics\" in best_exp and best_exp[\"metrics\"] is not None:\n",
    "                metrics = best_exp[\"metrics\"]\n",
    "                test_metrics = metrics[metrics[\"metric\"] == \"test_metric\"]\n",
    "                # Filter to only show up to MAX_ROUND\n",
    "                test_metrics = test_metrics[test_metrics[\"round\"] <= MAX_ROUND]\n",
    "                if not test_metrics.empty:\n",
    "                    round_means = test_metrics.groupby(\"round\")[\"value\"].mean()\n",
    "                    ax.plot(\n",
    "                        round_means.index,\n",
    "                        round_means.values,\n",
    "                        label=agg,\n",
    "                        linewidth=2,\n",
    "                        marker=\"o\",\n",
    "                        markersize=4,\n",
    "                        markevery=5,\n",
    "                        color=colors[idx],\n",
    "                    )\n",
    "\n",
    "    ax.set_xlabel(\"Round\", fontsize=12)\n",
    "    ax.set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "    ax.set_title(f\"Learning Curves - Best Configurations (Analysis up to Round {MAX_ROUND})\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlim([0, MAX_ROUND + 1])\n",
    "    # Force integer ticks on x-axis\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.legend(loc=\"lower right\", fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for learning curves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configuration Count Per Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not experiments_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "    config_counts = experiments_df.groupby(\"aggregator\").size().sort_values(ascending=False)\n",
    "    bars = ax.bar(range(len(config_counts)), config_counts.values, color=\"skyblue\", alpha=0.8, edgecolor=\"navy\", linewidth=1.5)\n",
    "\n",
    "    ax.set_xticks(range(len(config_counts)))\n",
    "    ax.set_xticklabels(config_counts.index, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Number of Configurations\", fontsize=12)\n",
    "    ax.set_title(\"Configurations Tested per Aggregator\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, config_counts.values):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f\"{value}\", ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for configuration count.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ranked Configurations Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not experiments_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    aggregators = sorted(experiments_df[\"aggregator\"].unique())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(aggregators)))\n",
    "\n",
    "    x_offset = 0\n",
    "    x_positions = []\n",
    "    x_labels = []\n",
    "\n",
    "    for agg_idx, agg in enumerate(aggregators):\n",
    "        agg_df = experiments_df[experiments_df[\"aggregator\"] == agg].sort_values(\"final_accuracy\", ascending=False)\n",
    "\n",
    "        if not agg_df.empty:\n",
    "            # Plot all configurations for this aggregator\n",
    "            n_configs = len(agg_df)\n",
    "            x_pos = np.arange(x_offset, x_offset + n_configs)\n",
    "\n",
    "            # Create bars with gradient effect (best configs darker)\n",
    "            alphas = np.linspace(1.0, 0.3, n_configs)\n",
    "            for i, (idx, row) in enumerate(agg_df.iterrows()):\n",
    "                bar = ax.bar(x_pos[i], row[\"final_accuracy\"], color=colors[agg_idx], alpha=alphas[i], edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "                # Add rank labels for top 3\n",
    "                if i < 3:\n",
    "                    ax.text(x_pos[i], row[\"final_accuracy\"] + 0.002, f\"#{i + 1}\", ha=\"center\", va=\"bottom\", fontsize=8, fontweight=\"bold\")\n",
    "\n",
    "            # Add aggregator label\n",
    "            x_positions.append(x_offset + n_configs / 2 - 0.5)\n",
    "            x_labels.append(agg)\n",
    "\n",
    "            # Update offset for next aggregator\n",
    "            x_offset += n_configs + 2\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(x_labels, rotation=0, ha=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "    ax.set_title(f\"All Configurations Ranked by Accuracy (Round {MAX_ROUND})\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "    ax.set_ylim([0, max(experiments_df[\"final_accuracy\"].max() + 0.05, 1.0)])\n",
    "\n",
    "    # Add horizontal line for overall best\n",
    "    overall_best = experiments_df[\"final_accuracy\"].max()\n",
    "    ax.axhline(y=overall_best, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=1, label=f\"Overall Best: {overall_best:.4f}\")\n",
    "\n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    legend_elements = [Patch(facecolor=colors[i], alpha=0.8, label=agg) for i, agg in enumerate(aggregators)]\n",
    "    legend_elements.append(\n",
    "        plt.Line2D([0], [0], color=\"red\", linestyle=\"--\", label=f\"Overall Best at Round {MAX_ROUND}: {overall_best:.4f}\")\n",
    "    )\n",
    "    ax.legend(handles=legend_elements, loc=\"upper right\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SUMMARY STATISTICS (AT ROUND {MAX_ROUND})\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for agg in aggregators:\n",
    "        agg_df = experiments_df[experiments_df[\"aggregator\"] == agg]\n",
    "        if not agg_df.empty:\n",
    "            print(f\"\\n{agg}:\")\n",
    "            print(f\"  Configurations tested: {len(agg_df)}\")\n",
    "            print(f\"  Best accuracy: {agg_df['final_accuracy'].max():.4f}\")\n",
    "            print(f\"  Worst accuracy: {agg_df['final_accuracy'].min():.4f}\")\n",
    "            print(f\"  Mean accuracy: {agg_df['final_accuracy'].mean():.4f}\")\n",
    "            print(f\"  Std deviation: {agg_df['final_accuracy'].std():.4f}\")\n",
    "else:\n",
    "    print(\"No experiments available for ranking plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple summary cell as requested\n",
    "\n",
    "if not experiments_df.empty:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"FINAL SUMMARY - BEST CONFIGURATION PER AGGREGATOR (AT ROUND {MAX_ROUND})\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "\n",
    "    for agg in sorted(experiments_df[\"aggregator\"].unique()):\n",
    "        agg_df = experiments_df[experiments_df[\"aggregator\"] == agg]\n",
    "        best_exp = agg_df.loc[agg_df[\"final_accuracy\"].idxmax()]\n",
    "\n",
    "        # Get parameter columns\n",
    "        exclude_cols = [\n",
    "            \"aggregator\",\n",
    "            \"final_accuracy\",\n",
    "            \"final_accuracy_std\",\n",
    "            \"best_accuracy\",\n",
    "            \"metrics\",\n",
    "            \"exp_dir\",\n",
    "            \"max_round_used\",\n",
    "            \"final_round\",\n",
    "        ]\n",
    "        param_cols = [col for col in best_exp.index if col not in exclude_cols]\n",
    "\n",
    "        # Build parameter string\n",
    "        param_str = \", \".join(\n",
    "            [f\"{p}={format_param_value(best_exp[p])}\" for p in sorted(param_cols) if p in best_exp and not pd.isna(best_exp[p])]\n",
    "        )\n",
    "\n",
    "        summary_data.append({\"Aggregator\": agg, \"Best Accuracy\": f\"{best_exp['final_accuracy']:.4f}\", \"Parameters\": param_str})\n",
    "\n",
    "        # Print summary\n",
    "        best_round = int(best_exp.get(\"best_round\", best_exp.get(\"final_round\", MAX_ROUND)))\n",
    "        print(f\"{agg}: {best_exp['final_accuracy']:.4f} (best @ round {best_round}) | {param_str}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"OVERALL WINNER (AT ROUND {MAX_ROUND})\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    best_overall = experiments_df.loc[experiments_df[\"final_accuracy\"].idxmax()]\n",
    "    print(f\"Aggregator: {best_overall['aggregator']}\")\n",
    "    print(f\"Accuracy: {best_overall['final_accuracy']:.4f}\")\n",
    "    best_round = int(best_overall.get(\"best_round\", best_overall.get(\"final_round\", MAX_ROUND)))\n",
    "    print(f\"Best achieved at round: {best_round}\")\n",
    "\n",
    "    # Print all parameters for the winner\n",
    "    print(\"Parameters:\")\n",
    "    exclude_cols = [\n",
    "        \"aggregator\",\n",
    "        \"final_accuracy\",\n",
    "        \"final_accuracy_std\",\n",
    "        \"best_accuracy\",\n",
    "        \"metrics\",\n",
    "        \"exp_dir\",\n",
    "        \"max_round_used\",\n",
    "        \"final_round\",\n",
    "        \"best_round\",\n",
    "    ]\n",
    "    for param in sorted(best_overall.index):\n",
    "        if param not in exclude_cols and not pd.isna(best_overall[param]):\n",
    "            print(f\"  {param}: {format_param_value(best_overall[param])}\")\n",
    "else:\n",
    "    print(\"No experiments found. Please run the grid search commands first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2pfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
